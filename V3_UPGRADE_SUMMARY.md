# ‚ú® TEMPO V3 UPGRADE COMPLETE

## What Changed

The encoder and decoder in `model/film.py` have been upgraded to **V3** with significantly deeper feature extraction.

### Before (V2)
- **1-2 ResBlocks per scale**
- ~10 total conv layers
- Simple FiLM temporal conditioning
- No attention mechanisms

### After (V3)
- **4-12 ResBlocks per scale** (default 6)
- ~56 total conv layers
- Enhanced temporal fusion with spatial attention
- Squeeze-and-Excitation (SE) channel attention
- Multi-dilation receptive fields (1, 2)
- Pre-activation design for better gradients

---

## Expected Results

| Metric | V2 (Current) | V3 (Expected) | Improvement |
|--------|--------------|---------------|-------------|
| **PSNR** | 32.0 dB | 34-35 dB | +2-3 dB |
| **SSIM** | 0.930 | 0.95-0.96 | +0.02-0.03 |
| **Parameters** | ~3M | ~12M | 4x larger |
| **Training Speed** | 1.0x | 0.6-0.7x | Slower but worth it |

---

## Testing

Run the test script to verify everything works:

```bash
python test_v3_upgrade.py
```

Expected output:
```
‚ú® FrameEncoder V3 initialized:
   - 6 blocks per scale
   - SE channel attention enabled
   - Multi-dilation receptive fields (1, 2)
   - Enhanced temporal fusion with spatial attention
   - Total conv layers: ~56

‚ú® DecoderTimeFiLM V3 initialized:
   - Progressive upsampling with refinement
   - 2-3 EnhancedResBlocks per upsampling stage
   - Enhanced temporal fusion at each scale

‚úÖ ALL TESTS PASSED - V3 ENCODER/DECODER WORKING CORRECTLY!
```

Or test with the original smoke test:

```bash
python smoke_test.py
```

---

## Training with V3

### Basic Training

```bash
python train_tempo.py \
    --data_root datasets/vimeo_triplet \
    --batch_size 2 \
    --epochs 100 \
    --exp_name "tempo_v3_baseline"
```

### Recommended Settings

```bash
python train_tempo.py \
    --data_root datasets/vimeo_triplet \
    --batch_size 2 \
    --lr 5e-5 \
    --epochs 100 \
    --amp \
    --amp_dtype bf16 \
    --exp_name "tempo_v3_bf16" \
    --use_wandb
```

### With Distributed Training (Multi-GPU)

```bash
torchrun --nproc_per_node=2 train_tempo.py \
    --distributed \
    --batch_size 2 \
    --amp \
    --amp_dtype bf16 \
    --exp_name "tempo_v3_ddp"
```

---

## Important Notes

### Memory Usage
V3 uses ~4x more parameters, so reduce batch size:
- **256x256:** batch_size 4 ‚Üí 2
- **448x256:** batch_size 2 ‚Üí 1  
- **512x512:** batch_size 1 (or use gradient accumulation)

### Training Speed
- V3 is ~1.5x slower per iteration
- **BUT** converges to better results faster overall
- Use `--amp --amp_dtype bf16` for 30-40% speedup

### Hyperparameters
Consider adjusting:
```python
learning_rate: 5e-5  # Lower than V2's 1e-4
warmup_steps: 2000   # Longer warmup
grad_clip: 0.5       # Tighter clipping
```

---

## What to Monitor

### During Training
1. **PSNR should increase steadily** past 32 dB (not plateau)
2. **SSIM should reach 0.95+** (vs 0.93 plateau in V2)
3. **Validation samples** should be noticeably sharper
4. **Edge preservation** should improve (check error maps)

### Expected Learning Curves

**V2 plateau:**
```
Epoch 20:  30 dB
Epoch 50:  31 dB
Epoch 100: 32 dB ‚Üê plateau
```

**V3 continued improvement:**
```
Epoch 20:  31 dB
Epoch 50:  33-34 dB
Epoch 100: 34-35 dB ‚Üê still improving
```

---

## Troubleshooting

### "CUDA out of memory"
```bash
# Solution 1: Reduce batch size
--batch_size 1

# Solution 2: Use gradient accumulation (add to train_tempo.py if needed)
# Solution 3: Use mixed precision
--amp --amp_dtype bf16

# Solution 4: Reduce model size (add to build_tempo in tempo.py)
--base_channels 48  # Instead of 64
```

### "Training too slow"
```bash
# Enable mixed precision (30-40% faster)
--amp --amp_dtype bf16

# Enable PyTorch 2.0 compile (10-20% faster, PyTorch ‚â•2.0)
--compile

# Use multiple GPUs
torchrun --nproc_per_node=2 train_tempo.py --distributed
```

### "Loss is NaN"
```bash
# Lower learning rate
--lr 2e-5

# Increase warmup
# Edit config/default.py: warmup_steps: 3000

# Use FP32 temporarily
# Remove --amp flag
```

---

## Architecture Details

### Enhanced ResBlock
- Pre-activation (Norm ‚Üí Act ‚Üí Conv)
- Squeeze-and-Excitation channel attention
- Multi-dilation (1, 2) for varied receptive fields
- Residual scaling (0.1x) for stable training

### Enhanced Temporal Fusion
- Improved FiLM with deeper MLPs
- Spatial attention to focus modulation
- Blends original and modulated features

### Residual Groups
- 6 ResBlocks per group (default)
- Alternating dilations: [1, 1, 2, 1, 2, 1]
- Learnable residual scaling

---

## Validation

After training, check:

1. **PSNR ‚â• 34 dB** on Vimeo-90K test set
2. **SSIM ‚â• 0.95** on Vimeo-90K test set
3. **Visual quality:** Sharper edges, better textures
4. **Temporal consistency:** Less flickering

Compare validation samples from V2 vs V3 side-by-side.

---

## Reverting (If Needed)

If V3 doesn't work as expected, you can easily revert by restoring the old `film.py` from git:

```bash
git checkout model/film.py
```

Or keep both versions and switch imports in `tempo.py`.

---

## Next Steps

Once V3 is working well (34-35 PSNR), consider:

1. **Temporal Transformer** - Replace sinusoidal encoding (expected +0.5-1 PSNR)
2. **Better attention** - Swin Transformer blocks (expected +0.3-0.5 PSNR)  
3. **Loss improvements** - Laplacian pyramid, edge-aware (expected +0.2-0.5 PSNR)
4. **Ensemble** - Average multiple checkpoints (expected +0.1-0.3 PSNR)

**Target:** 36-37 PSNR (SOTA level on Vimeo-90K)

---

## Questions?

Check:
1. Test passes: `python test_v3_upgrade.py`
2. Smoke test passes: `python smoke_test.py`
3. Training starts without errors
4. GPU memory usage (should be ~4x higher)
5. Validation PSNR improving past 32 dB

Good luck! üöÄ
